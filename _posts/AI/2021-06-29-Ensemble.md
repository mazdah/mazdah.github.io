---
layout: article
title: AI/ML(패스트 캠퍼스) - 앙상블(Ensemble)
excerpt: 앙상블의 특직과 종류
mathjax: true
tags:
  - [AI, ML, Scikit-learn, Regression, Ensemble, 앙상블]
categories: AI
comments: true
header:
  theme: dark
  background: 'linear-gradient(135deg, rgb(34, 139, 87), rgb(139, 34, 139))'
article_header:
  type: overlay
  theme: dark
  background_color: '#203028'
  background_image:
    gradient: 'linear-gradient(135deg, rgba(34, 139, 87 , .4), rgba(139, 34, 139, .4))'
    src: /ai.jpg
---

# 앙상블 (Ensemble)

---

- 여러개의 머신러닝 모델을 이용해 최적의 답을 찾아내는 기법
- 여러 모델을 이용하여 데이터를 학습하고, 모든 모델의 예측 결과를 평균하여 최종 예측
- 대체로 단일 모델보다 성능이 좋다.
- 모델이 가능한한 서로 독립적일 때 최고의 성능을 낸다. 따라서 각기 다른 알고리즘으로 학습 시키는 것이 좋다.

### 앙상블 기법의 종류

#### 보팅(Voting)
투표를 통해 결과 도출, 직접 투표 방식과 간접 투표 방식이 있다. 간접 투표 방식이 성능이 더 높다. 다만 모델이 확률을 예측할 수 있는 경우(predict_proba() 함수가 있는 경우) 간접 투표가 가능하다.

#### 베깅(Bagging, bootstrap aggregating)
같은 알고리즘을 사용하되 훈련 세트의 서브 세트를 무작위로 구성하여 각기 다르게 학습 시키는 방법.
이 과정에서 중복을 허용하여 샘플링하는 방법을 배깅이라고 한다.

#### 페이스팅(Pasting)
배깅과 동일하나 데이터를 중복을 허용하지 않고 샘플링하는 방식

> 배깅과 페이스팅은 분류인 경우 최종 단계에서 각 학습기의 통계적 최빈값을 도출하고 회귀인 경우
> 평균을 도출한다. 또한 동시에 다른 CPU 코어나 서버에서 병렬로 학습시킬 수 있다.

#### 부스팅(Boosting)
약한 학습기를 여러 개 연결하여 강한 학습기를 만드는 방법. 이전 오차를 보완하면서 가중치 부여.
에이다부스트,(AdaBoost) 그레디언트 부스팅(Gradient Boosting) 등의 방법이 있다.

#### 스태킹(Stacking, stacked generalization)
여러 모델을 기반으로 예측된 결과를 통해 meta 모델이 다시 한번 예측
